* Use databricks developer resources link

* Have JDK instead of JRE (for Maven etc)

* Download Anaconda for Python installation

* scala spark shell
  (1) $ spark-*/bin/spark-shell
  (2) scala>
  (3) scala> sc
  (4) scala> sc.master

* To correct networking errors
  (1) $ SPARK_LOCAL_IP=127.0.0.1 spark-*/bin/spark-shell // when spark launches, it starts a server for UI and needs IP address to bind to

* scala 
  (1) http://scala-lang.org/
  (2) crash course http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf
  
** sc.master
  - local -- 1 worker thread
  - local[k] -- k worker threads
  - spark://HOST:PORT -- spark standalone cluster
  - mesos://HOST:PORT -- mesos cluster

** To run scala examples, run sbt (scala build tool)
  - ../sbt/sbt assembly
  - ./bin/run-example SparkPi 2 local // 2 partitions, local cluster
  
** Look into Monte Carlo method for estimation examples

** Spark streaming example - TwitterAlgebirdHLL - TwittterAlgebirdCMS
  - have a twitter account
  - get an authentication token and use twitter4j library to put in properties file
  - details at twitter algebird wiki for approximation algorithm
  
** Shark --> spark on hive (hive optimizer not optimal) --> replaced with Spark SQL (optimizer called catalyst)
  - obj.registerTempTable("obj name")
  - create sql statements on obj
** Spark SQL has DSL comparable with SQL and LINQ

** Using HiveContext, queries can be written in HiveQL  

** Launching python notebook
  - IPYTHON_OPTS="notebook --pylab inline" ./bin/pyspark
  - Instead of using case class as in scala, in python data can be mapped to a Row object
  - sqlCtx.inferSchema(obj) // extra step in python before registerTempTable()
** Parquet files are columnar format
